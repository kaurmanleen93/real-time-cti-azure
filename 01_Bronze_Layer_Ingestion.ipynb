{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d914b8f9-0032-4824-92e5-b9f4c05dc3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#CELL 1 — Imports\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"✔ Libraries imported\")\n",
    "\n",
    "#CELL 2 — Paths\n",
    "raw_path = \"/Volumes/dbw_cti_processing/default/raw_data/ev_range_analysis_subset.csv\"\n",
    "bronze_path = \"/mnt/cti/bronze/ev_data\"\n",
    "\n",
    "print(\"Raw:\", raw_path)\n",
    "print(\"Bronze:\", bronze_path)\n",
    "\n",
    "#CELL 3 — File Check\n",
    "print(\"Checking file...\")\n",
    "\n",
    "try:\n",
    "    spark.read.csv(raw_path, header=True).limit(1).show()\n",
    "    print(\"✔ CSV file found.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ File missing:\", e)\n",
    "\n",
    "#CELL 4 — Read CSV\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(raw_path)\n",
    ")\n",
    "\n",
    "display(df_raw)\n",
    "df_raw.printSchema()\n",
    "\n",
    "print(\"✔ Records loaded:\", df_raw.count())\n",
    "\n",
    "#CELL 5 — Sample\n",
    "df_raw.show(5, truncate=False)\n",
    "\n",
    "#CELL 6 — No extraction needed\n",
    "df_processed = df_raw\n",
    "print(\"✔ Using CSV data as-is.\")\n",
    "\n",
    "#CELL 7 — Sanitize Columns\n",
    "def sanitize(df):\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.strip().replace(\" \", \"_\"))\n",
    "    return df\n",
    "\n",
    "df_clean = sanitize(df_processed)\n",
    "\n",
    "#CELL 8 — Generate Strong Unique record_id\n",
    "#Fix: SHA256 hash of all columns, ensuring no duplicates ever\n",
    "# Build record_id from all columns (safe)\n",
    "cols = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"NULL\")) for c in df_clean.columns]\n",
    "\n",
    "df_bronze_ready = (\n",
    "    df_clean\n",
    "        .withColumn(\n",
    "            \"record_id\",\n",
    "            F.sha2(F.concat_ws(\"||\", *cols), 256)\n",
    "        )\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"source\", F.lit(\"EV_Range_Data\"))\n",
    "        .withColumn(\"file_name\", F.lit(raw_path))\n",
    "        .withColumn(\"year\", F.year(F.current_timestamp()))\n",
    "        .withColumn(\"month\", F.month(F.current_timestamp()))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.current_timestamp()))\n",
    ")\n",
    "\n",
    "#CELL 9 — Deduplicate the source batch\n",
    "df_source = df_bronze_ready.dropDuplicates([\"record_id\"])\n",
    "\n",
    "print(\"Source before:\", df_bronze_ready.count())\n",
    "print(\"Source after dedup:\", df_source.count())\n",
    "\n",
    "#CELL 10 — Repair existing Bronze table (only if exists)\n",
    "#✔ Removes bad duplicate record_id from old table\n",
    "if DeltaTable.isDeltaTable(spark, bronze_path):\n",
    "    print(\"\uD83D\uDD27 Cleaning existing Bronze table...\")\n",
    "\n",
    "    df_target = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "    from pyspark.sql.window import Window\n",
    "    w = Window.partitionBy(\"record_id\").orderBy(F.col(\"ingestion_timestamp\").desc())\n",
    "\n",
    "    df_target_clean = (\n",
    "        df_target\n",
    "            .withColumn(\"rn\", F.row_number().over(w))\n",
    "            .filter(\"rn = 1\")\n",
    "            .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    print(\"Target before:\", df_target.count())\n",
    "    print(\"Target after dedup:\", df_target_clean.count())\n",
    "\n",
    "    (\n",
    "        df_target_clean.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .save(bronze_path)\n",
    "    )\n",
    "\n",
    "    print(\"✔ Bronze table repaired.\")\n",
    "else:\n",
    "    print(\"✔ No existing Bronze table found — skipping repair.\")\n",
    "\n",
    "#CELL 11 — Single MERGE (Clean Source → Clean Target)\n",
    "print(\"\uD83D\uDCBE Writing to Bronze...\")\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, bronze_path):\n",
    "    print(\"✔ MERGE into existing Bronze table\")\n",
    "\n",
    "    delta = DeltaTable.forPath(spark, bronze_path)\n",
    "\n",
    "    (\n",
    "        delta.alias(\"t\")\n",
    "            .merge(\n",
    "                df_source.alias(\"s\"),\n",
    "                \"t.record_id = s.record_id\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "    )\n",
    "\n",
    "    print(\"✔ MERGE completed.\")\n",
    "else:\n",
    "    print(\"✔ Creating new Bronze table\")\n",
    "\n",
    "    (\n",
    "        df_source.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"year\", \"month\", \"day\")\n",
    "            .save(bronze_path)\n",
    "    )\n",
    "\n",
    "    print(\"✔ New Bronze table created.\")\n",
    "\n",
    "#CELL 12 — Enable CDF\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE delta.`{bronze_path}`\n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "print(\"✔ CDF enabled.\")\n",
    "\n",
    "#CELL 13 — Summary\n",
    "df_verify = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "print(\"Total records:\", df_verify.count())\n",
    "df_verify.groupBy(\"source\").count().show()\n",
    "df_verify.groupBy(\"year\",\"month\",\"day\").count().show()\n",
    "\n",
    "#CELL 14 — Final Sample\n",
    "display(\n",
    "    df_verify.select(\n",
    "        \"Model_Year\", \"Make\", \"Model\", \"Electric_Range\",\n",
    "        \"latitude\", \"longitude\", \"ingestion_timestamp\"\n",
    "    )\n",
    ")\n",
    "print(\"✔ Bronze ingestion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e7aca5-2cf2-46cc-846c-a5f9251852c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_Layer_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}